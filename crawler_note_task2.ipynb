{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **二、学习内容**\n",
    "\n",
    "## **2.1 Beautiful Soup库入门**\n",
    "\n",
    "1. 学习beautifulsoup基础知识。\n",
    "\n",
    "2. 使用beautifulsoup解析HTML页面。\n",
    "\n",
    "    - Beautiful Soup 是一个HTML/XML 的解析器，主要用于解析和提取 HTML/XML 数据。 \n",
    "    - 它基于HTML DOM 的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。 \n",
    "    - BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。\n",
    "    - 虽然说BeautifulSoup4 简单容易比较上手，但是匹配效率还是远远不如正则以及xpath的，一般不推荐使用，推荐正则的使用。\n",
    " \n",
    "\n",
    "- 第一步：pip install beautifulsoup4 ，万事开头难，先安装 beautifulsoup4，安装成功后就完成了第一步。\n",
    "\n",
    "- 第二步：导入from bs4 import BeautifulSoup\n",
    "\n",
    "- 第三步：创建 Beautiful Soup对象   soup = BeautifulSoup(html，'html.parser') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 实战：中国大学排名定向爬取\n",
    "- 爬取url：http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html\n",
    "- 爬取思路：\n",
    "    1. 从网络上获取大学排名网页内容\n",
    "    2. 提取网页内容中信息到合适的数据结构（二维数组）-排名，学校名称，总分\n",
    "    3. 利用数据结构展示并输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 从网络上获取大学排名网页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get('http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 提取网页内容中信息到合适的数据结构（二维数组）\n",
    "1. 查看网页源代码，观察并定位到需要爬取内容的标签；\n",
    "2. 使用bs4的查找方法提取所需信息-'排名，学校名称，总分'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding = r.apparent_encoding#设置默认编码\n",
    "soup=BeautifulSoup(r.text,'html.parser')#转bs4，慢\n",
    "#(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 利用数据结构展示并输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['排名', '学校', '总分'],\n",
       " ['1', '清华大学', '94.6'],\n",
       " ['2', '北京大学', '76.5'],\n",
       " ['3', '浙江大学', '72.9'],\n",
       " ['4', '上海交通大学', '72.1'],\n",
       " ['5', '复旦大学', '65.6'],\n",
       " ['6', '中国科学技术大学', '60.9'],\n",
       " ['7', '华中科技大学', '58.9'],\n",
       " ['7', '南京大学', '58.9'],\n",
       " ['9', '中山大学', '58.2'],\n",
       " ['10', '哈尔滨工业大学', '56.7'],\n",
       " ['11', '北京航空航天大学', '56.3'],\n",
       " ['12', '武汉大学', '56.2'],\n",
       " ['13', '同济大学', '55.7'],\n",
       " ['14', '西安交通大学', '55.0'],\n",
       " ['15', '四川大学', '54.4'],\n",
       " ['16', '北京理工大学', '54.0'],\n",
       " ['17', '东南大学', '53.6'],\n",
       " ['18', '南开大学', '52.8'],\n",
       " ['19', '天津大学', '52.3'],\n",
       " ['20', '华南理工大学', '52.0'],\n",
       " ['21', '中南大学', '50.3'],\n",
       " ['22', '北京师范大学', '49.7'],\n",
       " ['23', '山东大学', '49.1'],\n",
       " ['23', '厦门大学', '49.1'],\n",
       " ['25', '吉林大学', '48.9'],\n",
       " ['26', '大连理工大学', '48.6'],\n",
       " ['27', '电子科技大学', '48.4'],\n",
       " ['28', '湖南大学', '48.1'],\n",
       " ['29', '苏州大学', '47.3'],\n",
       " ['30', '西北工业大学', '46.7']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schoollist=[['排名','学校','总分']]\n",
    "for school in soup.find_all(\"tr\",\"alt\"):\n",
    "    detaillist=[]\n",
    "    for detail in school.children:\n",
    "        detaillist.append(detail.text)\n",
    "    #print(detaillist[0],detaillist[1],detaillist[3])\n",
    "    schoollist.append([detaillist[0],detaillist[1],detaillist[3]])\n",
    "schoollist[0:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 学习xpath**\n",
    "\n",
    "### 2.2.1 学习目标：\n",
    "\n",
    "1. 学习xpath，使用lxml+xpath提取内容。\n",
    "\n",
    "2. 使用xpath提取丁香园论坛的回复内容。\n",
    "\n",
    "- 抓取丁香园网页：[http://www.dxy.cn/bbs/thread/626626#626626](http://www.dxy.cn/bbs/thread/626626#626626) 。\n",
    "\n",
    "\n",
    "### 2.2.2 Xpath常用的路径表达式：\n",
    "\n",
    "- XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。\n",
    "- 在XPath中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。\n",
    "- XML文档是被作为节点树来对待的。\n",
    "\n",
    "XPath使用路径表达式在XML文档中选取节点。节点是通过沿着路径选取的。下面列出了最常用的路径表达式：\n",
    "\n",
    "- nodename       选取此节点的所有子节点。\n",
    "- /           从根节点选取。\n",
    "- //\t        从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。\n",
    "- .\t           选取当前节点。\n",
    "- ..\t         选取当前节点的父节点。\n",
    "- @\t           选取属性。\n",
    "- /text()        提取标签下面的文本内容\n",
    "    - 如：\n",
    "    - /标签名               逐层提取\n",
    "    - /标签名               提取所有名为<>的标签\n",
    "    - //标签名[@属性=“属性值”]   提取包含属性为属性值的标签\n",
    "    - @属性名               代表取某个属性名的属性值\n",
    "    \n",
    "- 详细学习：https://www.cnblogs.com/gaojun/archive/2012/08/11/2633908.html\n",
    "\n",
    "### 2.2.3 使用lxml解析\n",
    "\n",
    "- 导入库：from lxml import etree\n",
    "\n",
    "- lxml将html文本转成xml对象\n",
    "    - tree = etree.HTML(html)\n",
    "    \n",
    "- 用户名称：tree.xpath(’//div[@class=“auth”]/a/text()’)\n",
    "- 回复内容：tree.xpath(’//td[@class=“postbody”]’) 因为回复内容中有换行等标签，所以需要用string()来获取数据。\n",
    "    - string()的详细见链接：https://www.cnblogs.com/CYHISTW/p/12312570.html\n",
    "    \n",
    "- Xpath中text()，string()，data()的区别如下：\n",
    "    - text()仅仅返回所指元素的文本内容。\n",
    "    - string()函数会得到所指元素的所有节点文本内容，这些文本讲会被拼接成一个字符串。\n",
    "    - data()大多数时候，data()函数和string()函数通用，而且不建议经常使用data()函数，有数据表明，该函数会影响XPath的性能。\n",
    "\n",
    "#### 3. 利用Xpath表达式获取user和content（完成xpath的语句）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = tree.xpath('//div[@class=\"auth\"]/a/text()')#text()看作节点直接调用\n",
    "print(user)\n",
    "content = tree.xpath('//td[@class=\"postbody\"]')#string(.)看作路径，需要.xpath二次“查询”\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 学习正则表达式 re**\n",
    "\n",
    "### 2.3.1 为什么使用正则表达式？\n",
    "典型的搜索和替换操作要求您提供与预期的搜索结果匹配的确切文本。虽然这种技术对于对静态文本执行简单搜索和替换任务可能已经足够了，但它缺乏灵活性，若采用这种方法搜索动态文本，即使不是不可能，至少也会变得很困难。\n",
    "\n",
    "通过使用正则表达式，可以：\n",
    "\n",
    "    - 模式验证。\n",
    "        例如，可以测试输入字符串，以查看字符串内是否出现电话号码模式或信用卡号码模式。这称为数据验证。\n",
    "    - 替换文本。\n",
    "        可以使用正则表达式来识别文档中的特定文本，完全删除该文本或者用其他文本替换它。\n",
    "    - 基于模式匹配从字符串中提取子字符串。\n",
    "        可以查找文档内或输入域内特定的文本。\n",
    "可以使用正则表达式来搜索和替换标记。\n",
    "\n",
    "### 2.3.2 正则表达式语法\n",
    "\n",
    "正则表达式语法由字符和操作符构成:\n",
    "\n",
    "- 常用操作符\n",
    "    - `.` 表示任何单个字符\n",
    "    - `[ ]` 字符集，对单个字符给出取值范围 ，如`[abc]`表示a、b、c，`[a‐z]`表示a到z单个字符\n",
    "    - `[^ ]` 非字符集，对单个字符给出排除范围 ，如`[^abc]`表示非a或b或c的单个字符\n",
    "    - `*` 前一个字符0次或无限次扩展，如abc* 表示 ab、abc、abcc、abccc等 \n",
    "    - `+` 前一个字符1次或无限次扩展 ，如abc+ 表示 abc、abcc、abccc等 \n",
    "    - `?` 前一个字符0次或1次扩展 ，如abc? 表示 ab、abc\n",
    "    - `|` 左右表达式任意一个 ，如abc|def 表示 abc、def\n",
    "\n",
    "    - `{m}` 扩展前一个字符m次 ，如ab{2}c表示abbc\n",
    "    - `{m,n}` 扩展前一个字符m至n次（含n） ，如ab{1,2}c表示abc、abbc\n",
    "    - `^` 匹配字符串开头 ，如^abc表示abc且在一个字符串的开头\n",
    "    - `$` 匹配字符串结尾 ，如abc$表示abc且在一个字符串的结尾\n",
    "    - `( )` 分组标记，内部只能使用 | 操作符 ，如(abc)表示abc，(abc|def)表示abc、def\n",
    "    - `\\d` 数字，等价于`[0‐9]`\n",
    "    - `\\w` 单词字符，等价于`[A‐Za‐z0‐9_]`\n",
    "    \n",
    "    \n",
    "### 2.3.3 正则表达式re库的使用\n",
    "\n",
    "- 调用方式：import re\n",
    "- re库采用raw string类型表示正则表达式，表示为：r'text'，raw string是不包含对转义符再次转义的字符串;（所见即所得）\n",
    "\n",
    "#### re库的主要功能函数：\n",
    "\n",
    "- re.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象 \n",
    "    - re.search(pattern, string, flags=0)\n",
    "- re.match() 从一个字符串的开始位置起匹配正则表达式，返回match对象\n",
    "    - re.match(pattern, string, flags=0)\n",
    "- re.findall() 搜索字符串，以列表类型返回全部能匹配的子串\n",
    "    - re.findall(pattern, string, flags=0)\n",
    "- re.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型\n",
    "    - re.split(pattern, string, maxsplit=0, flags=0)\n",
    "- re.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象\n",
    "    - re.finditer(pattern, string, flags=0)\n",
    "- re.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串\n",
    "    - re.sub(pattern, repl, string, count=0, flags=0)\n",
    "\n",
    "    -  flags : 正则表达式使用时的控制标记：\n",
    "        - re.I -->  re.IGNORECASE :  忽略正则表达式的大小写，`[A‐Z]`能够匹配小写字符\n",
    "        - re.M -->  re.MULTILINE :  正则表达式中的^操作符能够将给定字符串的每行当作匹配开始\n",
    "        - re.S -->  re.DOTALL   :  正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符\n",
    "        \n",
    "#### re库的另一种等价用法（编译）\n",
    "\n",
    "- regex = re.compile(pattern, flags=0)：将正则表达式的字符串形式编译成正则表达式对象\n",
    "\n",
    "#### re 库的贪婪匹配和最小匹配\n",
    "\n",
    "- `.*` Re库默认采用贪婪匹配，即输出匹配最长的子串\n",
    "- `*?` 只要长度输出可能不同的，都可以通过在操作符后增加?变成最小匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\\"view_price\\\"\\:\\\"[\\d\\.]*\\\"')#[\\d\\.]*匹配任意长度带小数点整数。不保证小数点数量《1\n",
    "#: . 在正则表达式里有特殊作用，当作普通标点匹配时加\\转义\n",
    "print( '\\\"raw_title\\\"\\:\\\".*?\\\"')#.*?惰性匹配，匹配满足首尾条件的最短字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def getHTMLText(url):\n",
    "    \"\"\"\n",
    "    请求获取html，（字符串）\n",
    "    :param url: 爬取网址\n",
    "    :return: 字符串\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 添加头信息,\n",
    "        kv = {\n",
    "            'cookie': 'thw=cn; v=0; t=ab66dffdedcb481f77fd563809639584; cookie2=1f14e41c704ef58f8b66ff509d0d122e; _tb_token_=5e6bed8635536; cna=fGOnFZvieDECAXWIVi96eKju; unb=1864721683; sg=%E4%B8%8B3f; _l_g_=Ug%3D%3D; skt=83871ef3b7a49a0f; cookie1=BqeGegkL%2BLUif2jpoUcc6t6Ogy0RFtJuYXR4VHB7W0A%3D; csg=3f233d33; uc3=vt3=F8dBy3%2F50cpZbAursCI%3D&id2=UondEBnuqeCnfA%3D%3D&nk2=u%2F5wdRaOPk21wDx%2F&lg2=VFC%2FuZ9ayeYq2g%3D%3D; existShop=MTU2MjUyMzkyMw%3D%3D; tracknick=%5Cu4E36%5Cu541B%5Cu4E34%5Cu4E3F%5Cu5929%5Cu4E0B; lgc=%5Cu4E36%5Cu541B%5Cu4E34%5Cu4E3F%5Cu5929%5Cu4E0B; _cc_=WqG3DMC9EA%3D%3D; dnk=%5Cu4E36%5Cu541B%5Cu4E34%5Cu4E3F%5Cu5929%5Cu4E0B; _nk_=%5Cu4E36%5Cu541B%5Cu4E34%5Cu4E3F%5Cu5929%5Cu4E0B; cookie17=UondEBnuqeCnfA%3D%3D; tg=0; enc=2GbbFv3joWCJmxVZNFLPuxUUDA7QTpES2D5NF0D6T1EIvSUqKbx15CNrsn7nR9g%2Fz8gPUYbZEI95bhHG8M9pwA%3D%3D; hng=CN%7Czh-CN%7CCNY%7C156; mt=ci=32_1; alitrackid=www.taobao.com; lastalitrackid=www.taobao.com; swfstore=97213; x=e%3D1%26p%3D*%26s%3D0%26c%3D0%26f%3D0%26g%3D0%26t%3D0%26__ll%3D-1%26_ato%3D0; uc1=cookie16=UtASsssmPlP%2Ff1IHDsDaPRu%2BPw%3D%3D&cookie21=UIHiLt3xThH8t7YQouiW&cookie15=URm48syIIVrSKA%3D%3D&existShop=false&pas=0&cookie14=UoTaGqj%2FcX1yKw%3D%3D&tag=8&lng=zh_CN; JSESSIONID=A502D8EDDCE7B58F15F170380A767027; isg=BMnJJFqj8FrUHowu4yKyNXcd2PXjvpa98f4aQWs-RbDvsunEs2bNGLfj8BYE6lWA; l=cBTDZx2mqxnxDRr0BOCanurza77OSIRYYuPzaNbMi_5dd6T114_OkmrjfF96VjWdO2LB4G2npwJ9-etkZ1QoqpJRWkvP.; whl=-1%260%260%261562528831082',\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'\n",
    "        }\n",
    "        r = requests.get(url, timeout=30, headers=kv)\n",
    "        # r = requests.get(url, timeout=30)\n",
    "        # print(r.status_code)#返回值200，通讯没问题；headers缺少任意一项导致返回都是Null，测试后被tb屏蔽\n",
    "        r.raise_for_status()#判断返回状态\n",
    "        r.encoding = r.apparent_encoding\n",
    "        \"\"\"\n",
    "        大多数情况下, apparent_encoding 更准\n",
    "        例子:百度首页的响应回头里 content-type 压根不说明编码, \n",
    "        requests 此时 encoding会返回默认的ISO-8859-1, \n",
    "        但其实编码应该是utf-8, 而apparent_encoding分析出来就是真实的utf-8\n",
    "        新浪如是京东也如是\n",
    "        所以这也是为什么爬虫里大多数会写 r.encoding = r.apparent_encoding\n",
    "        二. 建议apparent_encoding 分析编码是比较耗时的,\n",
    "        在爬取大量网页时, 如果事先确定知道网页的编码了, \n",
    "        可以自己手动设置r.encoding=\"编码\", 这样性能会提升：r.encoding=\"utf-8\"\n",
    "        \"\"\"\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"爬取失败\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePage(glist, html):#对字符串html进行正则表达式匹配，返回结果到glist\n",
    "    '''\n",
    "    解析网页，搜索需要的信息\n",
    "    :param glist: 列表作为存储容器\n",
    "    :param html: 由getHTMLText()得到的\n",
    "    :return: 商品信息的列表\n",
    "    '''\n",
    "    try:\n",
    "        # 使用正则表达式提取信息\n",
    "        price_list = re.findall(r'\\\"view_price\\\"\\:\\\"[\\d\\.]*\\\"', html)\n",
    "        name_list = re.findall(r'\\\"raw_title\\\"\\:\\\".*?\\\"', html)\n",
    "        for i in range(len(price_list)):\n",
    "            #eval是Python的一个内置函数，这个函数的作用是，返回传入字符串的表达式的结果。即变量赋值时，等号右边的表示是写成字符串的格式，返回值就是这个表达式的结果。\n",
    "            #字符串转字典、向用字符串表达的“操作”传递变量\n",
    "            price = eval(price_list[i].split(\":\")[1])  #eval（）在此可以去掉\"\"\n",
    "            name = eval(name_list[i].split(\":\")[1])\n",
    "            glist.append([price, name])\n",
    "    except:\n",
    "        print(\"解析失败\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
